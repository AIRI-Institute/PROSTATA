{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e10590fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TORCH_HOME=/ayb/vol2/home/dumerenkov/torch_hub\n"
     ]
    }
   ],
   "source": [
    "%env TORCH_HOME=/ayb/vol2/home/dumerenkov/torch_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14e770d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "import os\n",
    "import torch\n",
    "import math\n",
    "import esm\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aede66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESMForSingleMutation(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        self.esm1v, self.esm1v_alphabet = esm.pretrained.esm2_t33_650M_UR50D()        \n",
    "        self.classifier = nn.Linear(1280, 1)\n",
    "        self.const1 = torch.nn.Parameter(torch.ones((1,1280)))\n",
    "        self.const2 = torch.nn.Parameter(-1 * torch.ones((1,1280)))\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, token_ids1, token_ids2, pos):\n",
    "                \n",
    "        outputs1 = self.esm1v.forward(token_ids1, repr_layers=[33])['representations'][33]\n",
    "        outputs2 = self.esm1v.forward(token_ids2, repr_layers=[33])['representations'][33]\n",
    "        \n",
    "        outputs = self.const1 * outputs1[:,pos,:] + self.const2 * outputs2[:,pos,:]\n",
    "        \n",
    "        #outputs = self.merge(torch.stack(, outputs2[:,0,:]]))\n",
    "        #outputs = torch.cat([torch.mean(outputs, dim = 1), torch.max(outputs, dim = 1).values], dim = 1)\n",
    "        logits = self.classifier(outputs)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a1cb550",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        _, esm1v_alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "        self.esm1v_batch_converter = esm1v_alphabet.get_batch_converter()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        _, _, esm1b_batch_tokens1 = self.esm1v_batch_converter([('' , ''.join(self.df.iloc[idx]['wt_seq'])[:1022])])\n",
    "        _, _, esm1b_batch_tokens2 = self.esm1v_batch_converter([('' , ''.join(self.df.iloc[idx]['mut_seq'])[:1022])])\n",
    "        pos = self.df.iloc[idx]['pos']\n",
    "        return esm1b_batch_tokens1, esm1b_batch_tokens2, pos, torch.unsqueeze(torch.FloatTensor([self.df.iloc[idx]['ddg']]), 0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "642d008e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    tr_loss, tr_accuracy = 0, 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    tr_preds, tr_labels = [], []\n",
    "    # put model in training mode\n",
    "    model.train()\n",
    "    \n",
    "    for idx, batch in enumerate(training_loader):\n",
    "        input_ids1, input_ids2, pos, labels = batch            \n",
    "        input_ids1 = input_ids1[0].to(device)\n",
    "        input_ids2 = input_ids2[0].to(device)\n",
    "        #labels = labels.to(device)\n",
    "        #print(model.device, input_ids.device, labels.device)\n",
    "        #print(input_ids)\n",
    "        #print(input_ids.size())\n",
    "        logits = model(token_ids1 = input_ids1, token_ids2 = input_ids2, pos = pos).to('cpu')        \n",
    "        #print(outputs)\n",
    "        loss = torch.nn.functional.mse_loss(logits, labels)\n",
    "        #print(model(input_ids=ids, attention_mask=mask, labels=labels))\n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples += labels.size(0)\n",
    "        \n",
    "        #if idx % 10==0:\n",
    "        #    loss_step = tr_loss/nb_tr_steps\n",
    "        #    print(f\"Training loss per 10 training steps: {loss_step}\")\n",
    "               \n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            parameters=model.parameters(), max_norm=0.1\n",
    "        )\n",
    "        \n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    epoch_loss = tr_loss / nb_tr_steps\n",
    "    print(f\"Training loss epoch: {epoch_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa0a19ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, testing_loader):\n",
    "    # put model in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_examples, nb_eval_steps = 0, 0\n",
    "    eval_preds, eval_labels, eval_scores = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(testing_loader):\n",
    "            \n",
    "            input_ids1, input_ids2, pos, labels = batch            \n",
    "            input_ids1 = input_ids1[0].to(device)\n",
    "            input_ids2 = input_ids2[0].to(device)\n",
    "            labels = labels.to(device)\n",
    "            #print(model.device, input_ids.device, labels.device)\n",
    "            #print(input_ids)\n",
    "            #print(input_ids.size())\n",
    "            logits = model(token_ids1 = input_ids1, token_ids2 = input_ids2, pos = pos)        \n",
    "            #print(outputs)\n",
    "            loss = torch.nn.functional.mse_loss(logits, labels)\n",
    "            #print(model(input_ids=ids, attention_mask=mask, labels=labels))\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            nb_eval_steps += 1\n",
    "            nb_eval_examples += labels.size(0)\n",
    "        \n",
    "            #if idx % 10==0:\n",
    "            #    loss_step = eval_loss/nb_eval_steps\n",
    "            #    print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n",
    "             \n",
    "            eval_labels.extend(labels.cpu().detach())\n",
    "            eval_preds.extend(logits.cpu().detach())\n",
    "            \n",
    "  \n",
    "    labels = [id.item() for id in eval_labels]\n",
    "    predictions = [id.item() for id in eval_preds]\n",
    "    \n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    print(f\"Validation Loss: {eval_loss}\")\n",
    "\n",
    "    return labels, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd04a35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c7d22fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = [\n",
    "    (['S3488.csv'], 'ssym.csv'),\n",
    "    (['S3488.csv'], 'ssym_r.csv'),\n",
    "    (['S2648.csv', 'ACDC_varibench.csv'], 'ssym.csv'),\n",
    "    (['S2648.csv', 'ACDC_varibench.csv'], 'ssym_r.csv'),\n",
    "    (['S2648.csv', 'ACDC_varibench.csv'], 'myoglobin.csv'),\n",
    "    (['S2648.csv', 'ACDC_varibench.csv'], 'p53.csv'),    \n",
    "    (['S2648.csv'], 'ssym.csv'),\n",
    "    (['S2648.csv'], 'ssym_r.csv'),\n",
    "    (['S2648.csv'], 'myoglobin.csv'),\n",
    "    (['S2648.csv'], 'myoglobin_r.csv'),\n",
    "    (['S3421.csv'], 'ssym.csv'),\n",
    "    (['S3421.csv'], 'ssym_r.csv'),\n",
    "    (['S3421.csv'], 'myoglobin.csv'),\n",
    "    (['S3421.csv'], 'myoglobin_r.csv'), \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dded08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1b3805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ede8b90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68b68c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05da5c1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1714384e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9dfa131",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on  ['S3488.csv']\n",
      "Test on  ssym.csv\n",
      "Training loss epoch: 2.3765579482604955\n",
      "Validation Loss: 6.514057166844965\n",
      "Training loss epoch: 0.790747747586766\n",
      "Validation Loss: 7.606570858323418\n",
      "Training loss epoch: 0.19476715091208327\n",
      "Validation Loss: 7.941786421074331\n",
      "MAE 2.030546653857836 Correlation SpearmanrResult(correlation=-0.5590952175483324, pvalue=1.6315954600914708e-29)\n",
      "*****************\n",
      "Train on  ['S3488.csv']\n",
      "Test on  ssym_r.csv\n",
      "Training loss epoch: 2.2893451735836168\n",
      "Validation Loss: 7.074502755399578\n",
      "Training loss epoch: 0.6171288764961305\n",
      "Validation Loss: 7.9161272492797226\n",
      "Training loss epoch: 0.15353070672759242\n",
      "Validation Loss: 7.9343863616425425\n",
      "MAE 2.0373377312214402 Correlation SpearmanrResult(correlation=-0.5512651511893392, pvalue=1.4013630361929296e-28)\n",
      "*****************\n",
      "Train on  ['S2648.csv', 'ACDC_varibench.csv']\n",
      "Test on  ssym.csv\n",
      "Training loss epoch: 2.7818096098169347\n",
      "Validation Loss: 7.866931681285076\n",
      "Training loss epoch: 1.187477685575817\n",
      "Validation Loss: 10.480198418480855\n",
      "Training loss epoch: 0.563237158999125\n",
      "Validation Loss: 11.354866215949937\n",
      "MAE 2.4287028135078255 Correlation SpearmanrResult(correlation=-0.8676152572468648, pvalue=3.3763489638243857e-105)\n",
      "*****************\n",
      "Train on  ['S2648.csv', 'ACDC_varibench.csv']\n",
      "Test on  ssym_r.csv\n",
      "Training loss epoch: 2.897269310545643\n",
      "Validation Loss: 7.397760422336539\n",
      "Training loss epoch: 1.2623848714560542\n",
      "Validation Loss: 10.506105110057161\n",
      "Training loss epoch: 0.5327439831313233\n",
      "Validation Loss: 11.05838838697089\n",
      "MAE 2.441116072673198 Correlation SpearmanrResult(correlation=-0.8642419800911527, pvalue=1.7961351512386107e-103)\n",
      "*****************\n",
      "Train on  ['S2648.csv', 'ACDC_varibench.csv']\n",
      "Test on  myoglobin.csv\n",
      "Training loss epoch: 2.842886130447994\n",
      "Validation Loss: 4.403705292907387\n",
      "Training loss epoch: 1.2760471264323514\n",
      "Validation Loss: 3.719252176208753\n",
      "Training loss epoch: 0.6328507479562658\n",
      "Validation Loss: 4.116073419700657\n",
      "MAE 1.4964550973625104 Correlation SpearmanrResult(correlation=-0.6993649063961606, pvalue=5.525112155828432e-21)\n",
      "*****************\n",
      "Train on  ['S2648.csv', 'ACDC_varibench.csv']\n",
      "Test on  p53.csv\n",
      "Training loss epoch: 2.8742419100379983\n",
      "Validation Loss: 8.900779258223512\n",
      "Training loss epoch: 1.2690053176697804\n",
      "Validation Loss: 12.37240471621044\n",
      "Training loss epoch: 0.5805413237725762\n",
      "Validation Loss: 11.962386646662795\n",
      "MAE 2.5352535008319785 Correlation SpearmanrResult(correlation=-0.6641546134052418, pvalue=1.6229831252421867e-06)\n",
      "*****************\n",
      "Train on  ['S2648.csv']\n",
      "Test on  ssym.csv\n",
      "Training loss epoch: 2.5327293756630915\n",
      "Validation Loss: 7.512566388358364\n",
      "Training loss epoch: 1.0597234286877002\n",
      "Validation Loss: 9.412350016793013\n",
      "Training loss epoch: 0.41575993056063076\n",
      "Validation Loss: 10.476829088672764\n",
      "MAE 2.3886027113692454 Correlation SpearmanrResult(correlation=-0.7842853631766162, pvalue=1.7745315589353687e-72)\n",
      "*****************\n",
      "Train on  ['S2648.csv']\n",
      "Test on  ssym_r.csv\n",
      "Training loss epoch: 2.6828768672778756\n",
      "Validation Loss: 5.655313894630556\n",
      "Training loss epoch: 1.198687126322572\n",
      "Validation Loss: 8.256094794002442\n",
      "Training loss epoch: 0.4789773805219567\n",
      "Validation Loss: 9.242027421903998\n",
      "MAE 2.209447857980992 Correlation SpearmanrResult(correlation=-0.703331362993808, pvalue=2.4800820739239974e-52)\n",
      "*****************\n",
      "Train on  ['S2648.csv']\n",
      "Test on  myoglobin.csv\n",
      "Training loss epoch: 2.733651740784666\n",
      "Validation Loss: 5.102739174107988\n",
      "Training loss epoch: 1.1821635660101368\n",
      "Validation Loss: 3.785050744462629\n",
      "Training loss epoch: 0.4431578596119089\n",
      "Validation Loss: 3.5952414062838813\n",
      "MAE 1.3568345403660145 Correlation SpearmanrResult(correlation=-0.551778274656307, pvalue=4.844387153750621e-12)\n",
      "*****************\n",
      "Train on  ['S2648.csv']\n",
      "Test on  myoglobin_r.csv\n",
      "Training loss epoch: 2.6564116237529545\n",
      "Validation Loss: 5.097199380934263\n",
      "Training loss epoch: 1.1430338757745961\n",
      "Validation Loss: 4.107539514098918\n",
      "Training loss epoch: 0.4291467932543725\n",
      "Validation Loss: 4.067120119533583\n",
      "MAE 1.4614009011203228 Correlation SpearmanrResult(correlation=-0.5080337536309588, pvalue=3.7062003870155286e-10)\n",
      "*****************\n",
      "Train on  ['S3421.csv']\n",
      "Test on  ssym.csv\n",
      "Training loss epoch: 4.6180415603684875\n",
      "Validation Loss: 7.480789414427004\n",
      "Training loss epoch: 2.4966062400771905\n",
      "Validation Loss: 10.34079650620177\n",
      "Training loss epoch: 1.1195186698426494\n",
      "Validation Loss: 12.693046284766028\n",
      "MAE 2.5503577439424894 Correlation SpearmanrResult(correlation=-0.8574975264816624, pvalue=3.713662379253609e-100)\n",
      "*****************\n",
      "Train on  ['S3421.csv']\n",
      "Test on  ssym_r.csv\n",
      "Training loss epoch: 4.53867510903821\n",
      "Validation Loss: 9.081898241870107\n",
      "Training loss epoch: 2.3900002665449476\n",
      "Validation Loss: 12.474427850870976\n",
      "Training loss epoch: 1.1316991564327872\n",
      "Validation Loss: 12.461767518397098\n",
      "MAE 2.5574302053384 Correlation SpearmanrResult(correlation=-0.875789388572632, pvalue=1.384740275945703e-109)\n",
      "*****************\n",
      "Train on  ['S3421.csv']\n",
      "Test on  myoglobin.csv\n",
      "Training loss epoch: 4.577120390666255\n",
      "Validation Loss: 3.2590749024634738\n",
      "Training loss epoch: 2.287668416215465\n",
      "Validation Loss: 3.5674986563121345\n",
      "Training loss epoch: 1.1333025466456006\n",
      "Validation Loss: 3.933034599560605\n",
      "MAE 1.4591447454599191 Correlation SpearmanrResult(correlation=-0.6555655176705525, pvalue=8.30829318262006e-18)\n",
      "*****************\n",
      "Train on  ['S3421.csv']\n",
      "Test on  myoglobin_r.csv\n",
      "Training loss epoch: 4.589190575049023\n",
      "Validation Loss: 5.330133971228614\n",
      "Training loss epoch: 2.5633110956333485\n",
      "Validation Loss: 3.144633718911642\n",
      "Training loss epoch: 1.3252825430549497\n",
      "Validation Loss: 3.3208063929008342\n",
      "MAE 1.3417977737662479 Correlation SpearmanrResult(correlation=-0.6343017898334876, pvalue=1.9049380611657633e-16)\n",
      "*****************\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-5\n",
    "EPOCHS = 3\n",
    "device = 'cuda:2'\n",
    "                        \n",
    "for train_datasets, test_dataset in experiments:\n",
    "    print('Train on ', train_datasets)\n",
    "    print('Test on ', test_dataset)\n",
    "    \n",
    "    train_df = pd.concat([pd.read_csv(os.path.join('DATASETS', t)) for t in train_datasets])\n",
    "    test_df = pd.read_csv(os.path.join('DATASETS', test_dataset))\n",
    "    \n",
    "    train_ds, test_ds = ProteinDataset(train_df), ProteinDataset(test_df)\n",
    "    \n",
    "    training_loader = DataLoader(train_ds, batch_size=1, num_workers = 2, shuffle = True)\n",
    "    testing_loader = DataLoader(test_ds, batch_size=1, num_workers = 2)\n",
    "    \n",
    "    model = ESMForSingleMutation()\n",
    "    model.to(device)\n",
    "    #model.const1 = model.const1.to(device)\n",
    "    #model.const2 = model.const2.to(device)\n",
    "    \n",
    "    \n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, steps_per_epoch=len(training_loader), epochs=EPOCHS)\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        train(epoch)\n",
    "        labels, predictions = valid(model, testing_loader)\n",
    "    \n",
    "    print(f'MAE {np.mean(np.abs(np.array(labels) - np.array(predictions)))} Correlation {stats.spearmanr(labels, predictions)}')     \n",
    "    print('*****************')\n",
    "    del optimizer, scheduler, model\n",
    "    \n",
    "    #model.to('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dde5975c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE 1.3417977737662479 Correlation SpearmanrResult(correlation=-0.6343017898334876, pvalue=1.9049380611657633e-16)\n"
     ]
    }
   ],
   "source": [
    "print(f'MAE {np.mean(np.abs(np.array(labels) - np.array(predictions)))} Correlation {stats.spearmanr(labels, predictions)}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bc08ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = [\n",
    "    (['deepddg_train.csv'], 'deepddg_test.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eeb56de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on  ['deepddg_train.csv']\n",
      "Test on  deepddg_test.csv\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/ayb/vol2/home/dumerenkov/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/ayb/vol2/home/dumerenkov/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/ayb/vol2/home/dumerenkov/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_21833/1540760637.py\", line 11, in __getitem__\n    return esm1b_batch_tokens1, esm1b_batch_tokens2, pos, torch.unsqueeze(torch.FloatTensor([self.df.iloc[idx]['ddg']]), 0)\nValueError: too many dimensions 'str'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mOneCycleLR(optimizer, max_lr\u001b[38;5;241m=\u001b[39mlr, steps_per_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(training_loader), epochs\u001b[38;5;241m=\u001b[39mEPOCHS)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[0;32m---> 27\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     labels, predictions \u001b[38;5;241m=\u001b[39m valid(model, testing_loader)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMAE \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39mabs(np\u001b[38;5;241m.\u001b[39marray(labels) \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39marray(predictions)))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Correlation \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstats\u001b[38;5;241m.\u001b[39mspearmanr(labels, predictions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)     \n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# put model in training mode\u001b[39;00m\n\u001b[1;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(training_loader):\n\u001b[1;32m      9\u001b[0m     input_ids1, input_ids2, pos, labels \u001b[38;5;241m=\u001b[39m batch            \n\u001b[1;32m     10\u001b[0m     input_ids1 \u001b[38;5;241m=\u001b[39m input_ids1[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/ayb/vol2/home/dumerenkov/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/ayb/vol2/home/dumerenkov/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1376\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1375\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1376\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/ayb/vol2/home/dumerenkov/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1402\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1400\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1402\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1403\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/ayb/vol2/home/dumerenkov/anaconda3/lib/python3.9/site-packages/torch/_utils.py:461\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m--> 461\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mValueError\u001b[0m: Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/ayb/vol2/home/dumerenkov/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/ayb/vol2/home/dumerenkov/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/ayb/vol2/home/dumerenkov/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_21833/1540760637.py\", line 11, in __getitem__\n    return esm1b_batch_tokens1, esm1b_batch_tokens2, pos, torch.unsqueeze(torch.FloatTensor([self.df.iloc[idx]['ddg']]), 0)\nValueError: too many dimensions 'str'\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-5\n",
    "EPOCHS = 3\n",
    "device = 'cuda:2'\n",
    "                        \n",
    "for train_datasets, test_dataset in experiments:\n",
    "    print('Train on ', train_datasets)\n",
    "    print('Test on ', test_dataset)\n",
    "    \n",
    "    train_df = pd.concat([pd.read_csv(os.path.join('DATASETS', t)) for t in train_datasets])\n",
    "    test_df = pd.read_csv(os.path.join('DATASETS', test_dataset))\n",
    "    \n",
    "    train_ds, test_ds = ProteinDataset(train_df), ProteinDataset(test_df)\n",
    "    \n",
    "    training_loader = DataLoader(train_ds, batch_size=1, num_workers = 2, shuffle = True)\n",
    "    testing_loader = DataLoader(test_ds, batch_size=1, num_workers = 2)\n",
    "    \n",
    "    model = ESMForSingleMutation()\n",
    "    model.to(device)\n",
    "    #model.const1 = model.const1.to(device)\n",
    "    #model.const2 = model.const2.to(device)\n",
    "    \n",
    "    \n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, steps_per_epoch=len(training_loader), epochs=EPOCHS)\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        train(epoch)\n",
    "        labels, predictions = valid(model, testing_loader)\n",
    "    \n",
    "    print(f'MAE {np.mean(np.abs(np.array(labels) - np.array(predictions)))} Correlation {stats.spearmanr(labels, predictions)}')     \n",
    "    print('*****************')\n",
    "    del optimizer, scheduler, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea55c01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
