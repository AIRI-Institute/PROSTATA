{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e10590fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TORCH_HOME=/ayb/vol2/home/dumerenkov/torch_hub\n"
     ]
    }
   ],
   "source": [
    "%env TORCH_HOME=/ayb/vol2/home/dumerenkov/torch_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14e770d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "import os\n",
    "import torch\n",
    "import math\n",
    "import esm\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aede66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESMForSingleMutation_pos(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        self.esm1v, self.esm1v_alphabet = esm.pretrained.esm2_t33_650M_UR50D()        \n",
    "        self.classifier = nn.Linear(1280, 1)\n",
    "        self.const1 = torch.nn.Parameter(torch.ones((1,1280)))\n",
    "        self.const2 = torch.nn.Parameter(-1 * torch.ones((1,1280)))\n",
    "        \n",
    "\n",
    "    def forward(self, token_ids1, token_ids2, pos):                \n",
    "        outputs1 = self.esm1v.forward(token_ids1, repr_layers=[33])['representations'][33]\n",
    "        outputs2 = self.esm1v.forward(token_ids2, repr_layers=[33])['representations'][33]\n",
    "        outputs = self.const1 * outputs1[:,pos + 1,:] + self.const2 * outputs2[:,pos + 1,:]        \n",
    "        logits = self.classifier(outputs)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4918e684",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESMForSingleMutation_cls(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        self.esm1v, self.esm1v_alphabet = esm.pretrained.esm2_t33_650M_UR50D()        \n",
    "        self.classifier = nn.Linear(1280, 1)\n",
    "        self.const1 = torch.nn.Parameter(torch.ones((1,1280)))\n",
    "        self.const2 = torch.nn.Parameter(-1 * torch.ones((1,1280)))\n",
    "        \n",
    "\n",
    "    def forward(self, token_ids1, token_ids2, pos):                \n",
    "        outputs1 = self.esm1v.forward(token_ids1, repr_layers=[33])['representations'][33]\n",
    "        outputs2 = self.esm1v.forward(token_ids2, repr_layers=[33])['representations'][33]\n",
    "        outputs = self.const1 * outputs1[:,0,:] + self.const2 * outputs2[:,0,:]        \n",
    "        logits = self.classifier(outputs)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a1cb550",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        _, esm1v_alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "        self.esm1v_batch_converter = esm1v_alphabet.get_batch_converter()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        _, _, esm1b_batch_tokens1 = self.esm1v_batch_converter([('' , ''.join(self.df.iloc[idx]['wt_seq'])[:1022])])\n",
    "        _, _, esm1b_batch_tokens2 = self.esm1v_batch_converter([('' , ''.join(self.df.iloc[idx]['mut_seq'])[:1022])])\n",
    "        pos = self.df.iloc[idx]['pos']\n",
    "        return esm1b_batch_tokens1, esm1b_batch_tokens2, pos, torch.unsqueeze(torch.FloatTensor([self.df.iloc[idx]['ddg']]), 0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "642d008e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    tr_loss, tr_accuracy = 0, 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    tr_preds, tr_labels = [], []\n",
    "    model.train()\n",
    "    \n",
    "    for idx, batch in enumerate(training_loader):\n",
    "        input_ids1, input_ids2, pos, labels = batch            \n",
    "        input_ids1 = input_ids1[0].to(device)\n",
    "        input_ids2 = input_ids2[0].to(device)\n",
    "        logits = model(token_ids1 = input_ids1, token_ids2 = input_ids2, pos = pos).to('cpu')        \n",
    "        loss = torch.nn.functional.mse_loss(logits, labels)\n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples += labels.size(0)\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            parameters=model.parameters(), max_norm=0.1\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    epoch_loss = tr_loss / nb_tr_steps\n",
    "    print(f\"Training loss epoch: {epoch_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa0a19ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, testing_loader):\n",
    "    model.eval()\n",
    "    \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_examples, nb_eval_steps = 0, 0\n",
    "    eval_preds, eval_labels, eval_scores = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(testing_loader):\n",
    "            \n",
    "            input_ids1, input_ids2, pos, labels = batch            \n",
    "            input_ids1 = input_ids1[0].to(device)\n",
    "            input_ids2 = input_ids2[0].to(device)\n",
    "            labels = labels.to(device)\n",
    "            logits = model(token_ids1 = input_ids1, token_ids2 = input_ids2, pos = pos)        \n",
    "            loss = torch.nn.functional.mse_loss(logits, labels)\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            nb_eval_steps += 1\n",
    "            nb_eval_examples += labels.size(0)\n",
    "                     \n",
    "            eval_labels.extend(labels.cpu().detach())\n",
    "            eval_preds.extend(logits.cpu().detach())\n",
    "            \n",
    "  \n",
    "    labels = [id.item() for id in eval_labels]\n",
    "    predictions = [id.item() for id in eval_preds]\n",
    "    \n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    print(f\"Validation Loss: {eval_loss}\")\n",
    "\n",
    "    return labels, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd04a35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c7d22fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = [\n",
    "    (['S2648.csv'], ['ssym.csv', 'ssym_r.csv', 'myoglobin.csv', 'myoglobin_r.csv']),\n",
    "    (['S3421.csv'], ['ssym.csv', 'ssym_r.csv', 'myoglobin.csv', 'myoglobin_r.csv']),\n",
    "    (['S3488.csv'], ['ssym.csv', 'ssym_r.csv']),\n",
    "    (['S2648.csv', 'ACDC_varibench.csv'], ['ssym.csv', 'ssym_r.csv', 'myoglobin.csv', 'p53.csv']),\n",
    "    (['deepddg_train.csv'], 'deepddg_test.csv')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a00d92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dfa131",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on  ['S2648.csv']\n",
      "Training model 0\n",
      "Training loss epoch: 2.5486238524853078\n",
      "Training loss epoch: 0.9919666604479591\n",
      "Training loss epoch: 0.38118088802880873\n",
      "\n",
      "Test on  ssym.csv\n",
      "Validation Loss: 0.9462949894599406\n",
      "MAE 0.6437292271307316 Correlation SpearmanrResult(correlation=0.7837955995257001, pvalue=2.49183792994039e-72)\n",
      "Test on  ssym_r.csv\n",
      "Validation Loss: 0.9584759757189668\n",
      "MAE 0.6488666684374932 Correlation SpearmanrResult(correlation=0.7836012845967981, pvalue=2.8504199243205333e-72)\n",
      "Test on  myoglobin.csv\n",
      "Validation Loss: 0.7291574193528808\n",
      "MAE 0.5926445554941893 Correlation SpearmanrResult(correlation=0.644766556664865, pvalue=4.205138302846179e-17)\n",
      "Test on  myoglobin_r.csv\n",
      "Validation Loss: 0.7290051383196774\n",
      "MAE 0.5900852801486739 Correlation SpearmanrResult(correlation=0.6446169174823382, pvalue=4.298742671163432e-17)\n",
      "\n",
      "Training model 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38208/1801029715.py:12: UserWarning: Using a target size (torch.Size([1, 1, 1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = torch.nn.functional.mse_loss(logits, labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss epoch: 2.965374656697283\n",
      "Training loss epoch: 1.9291575581249363\n",
      "Training loss epoch: 1.000102159788676\n",
      "\n",
      "Test on  ssym.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38208/3603313833.py:16: UserWarning: Using a target size (torch.Size([1, 1, 1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = torch.nn.functional.mse_loss(logits, labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.0555514228278529\n",
      "MAE 0.706959012574489 Correlation SpearmanrResult(correlation=0.7574394428100879, pvalue=6.3780581430158415e-65)\n",
      "Test on  ssym_r.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38208/3603313833.py:16: UserWarning: Using a target size (torch.Size([1, 1, 1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = torch.nn.functional.mse_loss(logits, labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.2880453322543342\n",
      "MAE 0.7998354901526973 Correlation SpearmanrResult(correlation=0.7501494068982905, pvalue=4.87740599967887e-63)\n",
      "Test on  myoglobin.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38208/3603313833.py:16: UserWarning: Using a target size (torch.Size([1, 1, 1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = torch.nn.functional.mse_loss(logits, labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6544752783552958\n",
      "MAE 0.568952531067293 Correlation SpearmanrResult(correlation=0.735814517273325, pvalue=4.212772686578414e-24)\n",
      "Test on  myoglobin_r.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38208/3603313833.py:16: UserWarning: Using a target size (torch.Size([1, 1, 1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = torch.nn.functional.mse_loss(logits, labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.8323934726527589\n",
      "MAE 0.6367423246239325 Correlation SpearmanrResult(correlation=0.7356174923496647, pvalue=4.3933549488328215e-24)\n",
      "\n",
      "Ensemble result for dataset ssym.csv\n",
      "Correlation SpearmanrResult(correlation=0.8028878290535527, pvalue=2.2207225171015702e-78)\n",
      "MSE 0.9426828407500886\n",
      "MAE 0.6247719830815675\n",
      "Ensemble result for dataset ssym_r.csv\n",
      "Correlation SpearmanrResult(correlation=0.8020883619174985, pvalue=4.1022157161219596e-78)\n",
      "MSE 0.9982676618564813\n",
      "MAE 0.6673528479040486\n",
      "Ensemble result for dataset myoglobin.csv\n",
      "Correlation SpearmanrResult(correlation=0.7675579758600201, pvalue=2.863625760803683e-27)\n",
      "MSE 0.7939569765127532\n",
      "MAE 0.5520670894783601\n",
      "Ensemble result for dataset myoglobin_r.csv\n",
      "Correlation SpearmanrResult(correlation=0.7679420497618389, pvalue=2.603340618590685e-27)\n",
      "MSE 0.8213995229064927\n",
      "MAE 0.554327963013897\n",
      "*****************\n",
      "\n",
      "Train on  ['S3421.csv']\n",
      "Training model 0\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-5\n",
    "EPOCHS = 3\n",
    "device = 'cuda:2'\n",
    "                        \n",
    "for train_datasets, test_datasets in experiments:\n",
    "    print('Train on ', train_datasets)\n",
    "    train_df = pd.concat([pd.read_csv(os.path.join('DATASETS', t)) for t in train_datasets])\n",
    "    train_ds = ProteinDataset(train_df)\n",
    "    \n",
    "    models = [ESMForSingleMutation_pos(), ESMForSingleMutation_cls()]\n",
    "    all_preds = {}\n",
    "    all_true = {}\n",
    "    for model_no, model in enumerate(models):    \n",
    "        model.to(device) \n",
    "        optimizer = torch.optim.Adam(params=model.parameters(), lr=lr)\n",
    "        training_loader = DataLoader(train_ds, batch_size=1, num_workers = 2, shuffle = True)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, steps_per_epoch=len(training_loader), epochs=EPOCHS)\n",
    "        \n",
    "\n",
    "        print(f'Training model {model_no}')\n",
    "        for epoch in range(EPOCHS):\n",
    "            train(epoch)\n",
    "        print()\n",
    "        \n",
    "        for test_no, test_dataset in enumerate(test_datasets):\n",
    "            if test_no not in all_preds:\n",
    "                all_preds[test_no] = {}\n",
    "            print('Test on ', test_dataset)\n",
    "            test_df = pd.read_csv(os.path.join('DATASETS', test_dataset))\n",
    "            test_ds = ProteinDataset(test_df)\n",
    "            testing_loader = DataLoader(test_ds, batch_size=1, num_workers = 2)\n",
    "            labels, predictions = valid(model, testing_loader)\n",
    "            print(f'MAE {np.mean(np.abs(np.array(labels) - np.array(predictions)))} Correlation {stats.spearmanr(labels, predictions)}')     \n",
    "            all_preds[test_no][model_no] = predictions\n",
    "            all_true[test_no] = labels\n",
    "            \n",
    "            \n",
    "        print()\n",
    "    \n",
    "    for test_idx in all_true.keys():\n",
    "        print(f'Ensemble result for dataset {test_datasets[test_idx]}')\n",
    "        ens_preds = np.mean(np.stack([all_preds[test_idx][t] for t in all_preds[test_idx].keys()], axis = -1), axis = -1)\n",
    "\n",
    "        print(f'Correlation {stats.spearmanr(all_true[test_idx], ens_preds)}')\n",
    "        print(f'MSE {np.sqrt(np.mean((np.array(ens_preds)-np.array(all_true[test_idx]))**2))}')        \n",
    "        print(f'MAE {np.mean(np.abs(np.array(all_true[test_idx]) - np.array(ens_preds)))}')    \n",
    "    \n",
    "    print('*****************')\n",
    "    print()\n",
    "    del optimizer, scheduler, model\n",
    "    \n",
    "    #model.to('cpu')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
