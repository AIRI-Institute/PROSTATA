{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e10590fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TORCH_HOME=/ayb/vol2/home/dumerenkov/torch_hub\n"
     ]
    }
   ],
   "source": [
    "%env TORCH_HOME=/ayb/vol2/home/dumerenkov/torch_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14e770d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "import os\n",
    "import torch\n",
    "import math\n",
    "import esm\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9aede66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESMForSingleMutation(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        self.esm1v, self.esm1v_alphabet = esm.pretrained.esm2_t33_650M_UR50D()        \n",
    "        self.classifier = nn.Linear(1280, 1)\n",
    "        self.const1 = torch.nn.Parameter(torch.ones((1,1280)))\n",
    "        self.const2 = torch.nn.Parameter(-1 * torch.ones((1,1280)))\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, token_ids1, token_ids2, pos):\n",
    "                \n",
    "        outputs1 = self.esm1v.forward(token_ids1, repr_layers=[33])['representations'][33]\n",
    "        outputs2 = self.esm1v.forward(token_ids2, repr_layers=[33])['representations'][33]\n",
    "        \n",
    "        outputs = self.const1 * outputs1[:,pos,:] + self.const2 * outputs2[:,pos,:]\n",
    "        \n",
    "        #outputs = self.merge(torch.stack(, outputs2[:,0,:]]))\n",
    "        #outputs = torch.cat([torch.mean(outputs, dim = 1), torch.max(outputs, dim = 1).values], dim = 1)\n",
    "        logits = self.classifier(outputs)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a1cb550",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        _, esm1v_alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "        self.esm1v_batch_converter = esm1v_alphabet.get_batch_converter()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        _, _, esm1b_batch_tokens1 = self.esm1v_batch_converter([('' , ''.join(self.df.iloc[idx]['wt_seq'])[:1022])])\n",
    "        _, _, esm1b_batch_tokens2 = self.esm1v_batch_converter([('' , ''.join(self.df.iloc[idx]['mut_seq'])[:1022])])\n",
    "        pos = self.df.iloc[idx]['pos']\n",
    "        return esm1b_batch_tokens1, esm1b_batch_tokens2, pos, torch.unsqueeze(torch.FloatTensor([self.df.iloc[idx]['ddg']]), 0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642d008e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa0a19ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, testing_loader):\n",
    "    # put model in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_examples, nb_eval_steps = 0, 0\n",
    "    eval_preds, eval_labels, eval_scores = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(testing_loader):\n",
    "            \n",
    "            input_ids1, input_ids2, pos, labels = batch            \n",
    "            input_ids1 = input_ids1[0].to(device)\n",
    "            input_ids2 = input_ids2[0].to(device)\n",
    "            labels = labels.to(device)\n",
    "            #print(model.device, input_ids.device, labels.device)\n",
    "            #print(input_ids)\n",
    "            #print(input_ids.size())\n",
    "            logits = model(token_ids1 = input_ids1, token_ids2 = input_ids2, pos = pos)        \n",
    "            #print(outputs)\n",
    "            loss = torch.nn.functional.mse_loss(logits, labels)\n",
    "            #print(model(input_ids=ids, attention_mask=mask, labels=labels))\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            nb_eval_steps += 1\n",
    "            nb_eval_examples += labels.size(0)\n",
    "        \n",
    "            #if idx % 10==0:\n",
    "            #    loss_step = eval_loss/nb_eval_steps\n",
    "            #    print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n",
    "             \n",
    "            eval_labels.extend(labels.cpu().detach())\n",
    "            eval_preds.extend(logits.cpu().detach())\n",
    "            \n",
    "  \n",
    "    labels = [id.item() for id in eval_labels]\n",
    "    predictions = [id.item() for id in eval_preds]\n",
    "    \n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    print(f\"Validation Loss: {eval_loss}\")\n",
    "\n",
    "    return labels, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd04a35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7d22fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8ee0949",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('ddg_v1.csv') \n",
    "\n",
    "train_df = data[data.dataset == 's2648']\n",
    "test_df = data[data.dataset == 's669']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9dfa131",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss epoch: 2.639705097934781\n",
      "Validation Loss: 2.3335013400976914\n",
      "Training loss epoch: 1.1393646466857565\n",
      "Validation Loss: 2.1834155305653247\n",
      "Training loss epoch: 0.46742047694741345\n",
      "Validation Loss: 2.1480892694312606\n",
      "*****************\n",
      "random state: 0 Correlation SpearmanrResult(correlation=0.5081393279348888, pvalue=3.3224215903824654e-45)\n",
      "Training loss epoch: 2.692595804584385\n",
      "Validation Loss: 2.358534636169595\n",
      "Training loss epoch: 1.170267046882815\n",
      "Validation Loss: 2.1826430898481406\n",
      "Training loss epoch: 0.4346870369894481\n",
      "Validation Loss: 2.1457223346142653\n",
      "*****************\n",
      "random state: 1 Correlation SpearmanrResult(correlation=0.5046421972690767, pvalue=1.6380419683838082e-44)\n",
      "Training loss epoch: 2.6936730900981045\n",
      "Validation Loss: 2.33386447071174\n",
      "Training loss epoch: 1.0799071757727707\n",
      "Validation Loss: 2.1721827564609932\n",
      "Training loss epoch: 0.3799153778997508\n",
      "Validation Loss: 2.1661947834665205\n",
      "*****************\n",
      "random state: 2 Correlation SpearmanrResult(correlation=0.49218358285332575, pvalue=4.1543149111749915e-42)\n",
      "Training loss epoch: 2.7779806191615615\n",
      "Validation Loss: 2.358369974468636\n",
      "Training loss epoch: 1.1555463665411483\n",
      "Validation Loss: 2.1682679529929385\n",
      "Training loss epoch: 0.4372046587646733\n",
      "Validation Loss: 2.169657690061502\n",
      "*****************\n",
      "random state: 3 Correlation SpearmanrResult(correlation=0.4922648215303403, pvalue=4.0099719373796405e-42)\n",
      "Training loss epoch: 2.652888696459685\n",
      "Validation Loss: 2.291085167547251\n",
      "Training loss epoch: 1.1781151814205861\n",
      "Validation Loss: 2.2215647776378917\n",
      "Training loss epoch: 0.4692331043823706\n",
      "Validation Loss: 2.203922292014109\n",
      "*****************\n",
      "random state: 4 Correlation SpearmanrResult(correlation=0.49223834982527465, pvalue=4.0564509853671984e-42)\n",
      "0.4978936558825812 0.007025493625573726\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-5\n",
    "EPOCHS = 3\n",
    "device = 'cuda:2'\n",
    "\n",
    "spearmanrs=[]\n",
    "                        \n",
    "for random_state in range(5):                        \n",
    "    \n",
    "    train_ds, test_ds = ProteinDataset(train_df), ProteinDataset(test_df)\n",
    "    \n",
    "    training_loader = DataLoader(train_ds, batch_size=1, num_workers = 2, shuffle = True)\n",
    "    testing_loader = DataLoader(test_ds, batch_size=1, num_workers = 2)\n",
    "    \n",
    "    model = ESMForSingleMutation()\n",
    "    model.to(device)\n",
    "    #model.const1 = model.const1.to(device)\n",
    "    #model.const2 = model.const2.to(device)\n",
    "    \n",
    "    \n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, steps_per_epoch=len(training_loader), epochs=EPOCHS)\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        train(epoch)\n",
    "        labels, predictions = valid(model, testing_loader)\n",
    "    print('*****************')\n",
    "    print(f'random state: {random_state} Correlation {stats.spearmanr(labels, predictions)}') \n",
    "    spearmanrs.append(stats.spearmanr(labels, predictions)[0])\n",
    "    del optimizer, scheduler, model\n",
    "    \n",
    "    #model.to('cpu')\n",
    "print(np.mean(spearmanrs), np.std(spearmanrs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde5975c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae159913",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    tr_loss, tr_accuracy = 0, 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    tr_preds, tr_labels = [], []\n",
    "    # put model in training mode\n",
    "    model.train()\n",
    "    \n",
    "    for idx, batch in enumerate(training_loader):\n",
    "        input_ids1, input_ids2, pos, labels = batch            \n",
    "        input_ids1 = input_ids1[0].to(device)\n",
    "        input_ids2 = input_ids2[0].to(device)\n",
    "        #labels = labels.to(device)\n",
    "        #print(model.device, input_ids.device, labels.device)\n",
    "        #print(input_ids)\n",
    "        #print(input_ids.size())\n",
    "        logits = model(token_ids1 = input_ids1, token_ids2 = input_ids2, pos = pos).to('cpu')        \n",
    "        #print(outputs)\n",
    "        loss = torch.nn.functional.mse_loss(logits, labels)\n",
    "        #print(model(input_ids=ids, attention_mask=mask, labels=labels))\n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples += labels.size(0)\n",
    "        \n",
    "        #if idx % 10==0:\n",
    "        #    loss_step = tr_loss/nb_tr_steps\n",
    "        #    print(f\"Training loss per 10 training steps: {loss_step}\")\n",
    "               \n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            parameters=model.parameters(), max_norm=0.1\n",
    "        )\n",
    "        \n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    epoch_loss = tr_loss / nb_tr_steps\n",
    "    print(f\"Training loss epoch: {epoch_loss}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
